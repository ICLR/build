<!doctype html>
<html lang="en">
  
  <head>
    <!-- Required meta tags -->
    <meta charset="utf-8">
    <meta name="viewport"
          content="width=device-width, initial-scale=1, shrink-to-fit=no">

    <link rel="stylesheet" href="static/css/main.css">

    <script src="https://d3js.org/d3.v5.min.js"></script>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/handlebars.js/4.7.3/handlebars.min.js"
            integrity="sha256-/PJBs6QWvXijOFIX04kZpLb6ZtSQckdOIavLWKKOgXU="
            crossorigin="anonymous"></script>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js"
            integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo="
            crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js" integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo" crossorigin="anonymous"></script>


    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/js/bootstrap.min.js" integrity="sha384-wfSDF2E50Y2D1uUdj0O3uMBJnjuUD4Ih7YwaYd1iqfktj0Uod8GCExl3Og8ifwB6" crossorigin="anonymous"></script>


    <script src="static/js/typeahead.bundle.js"></script>

    <link rel="stylesheet" href="static/css/typeahead.css">
    <link rel="stylesheet"
          href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css"
          integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T"
          crossorigin="anonymous">

    <title> ICLR: Tree-Structured Attention with Hierarchical Accumulation </title>
</head>


  <body >

<!-- NAV -->

<script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js" integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/js/bootstrap.min.js" integrity="sha384-wfSDF2E50Y2D1uUdj0O3uMBJnjuUD4Ih7YwaYd1iqfktj0Uod8GCExl3Og8ifwB6" crossorigin="anonymous"></script>
<link href="https://fonts.googleapis.com/css?family=Lato:400,900&display=swap" rel="stylesheet">
<link href="https://fonts.googleapis.com/css?family=Exo" rel='stylesheet'>
<link href="https://fonts.googleapis.com/css?family=Cuprum" rel='stylesheet'>
<nav class="navbar sticky-top navbar-expand-lg navbar-light bg-light mr-auto " id="main-nav" >
  <div class="container">
    <a class="navbar-brand" href="index.html">
      <img class="logo" style='visibility: ' src="static/images/ICLR-logo.png"  width="180px" />

    </a>
    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>
    <div class="collapse navbar-collapse text-right flex-grow-1" id="navbarNav">
      <ul class="navbar-nav ml-auto">
        
        <li class="nav-item ">
          <a class="nav-link"    href="index.html">Home</a>
        </li>
        
        <li class="nav-item ">
          <a class="nav-link"    href="calendar.html">Schedule</a>
        </li>
        
        <li class="nav-item ">
          <a class="nav-link"    href="workshops.html">Workshops</a>
        </li>
        
        <li class="nav-item ">
          <a class="nav-link"    href="papers.html">Papers</a>
        </li>
        
        <li class="nav-item ">
          <a class="nav-link" target="_blank"   href="https://iclr.6connex.com/event/VirtualEvent/">Sponsor Hall</a>
        </li>
        
        <li class="nav-item ">
          <a class="nav-link"    href="socials.html">Socials</a>
        </li>
        
        <li class="nav-item ">
          <a class="nav-link"    href="chat.html">Chat</a>
        </li>
        
        <li class="nav-item ">
          <a class="nav-link"    href="about.html">Help</a>
        </li>
        
      </ul>
    </div>
  </div>
</nav>


<div class="container">

 <!-- Title -->

 <div class="pp-card m-3" style="">
   <div class="card-header">
     <h2 class="card-title main-title text-center" style="">
       Tree-Structured Attention with Hierarchical Accumulation
     </h2>

     <h3 class="card-subtitle mb-2 text-muted text-center">
       
       Xuan-Phi Nguyen,
       
       Shafiq Joty,
       
       Steven Hoi,
       
       Richard Socher
       
     </h3>

     <div class="text-center p-3">
       <a class="card-link" data-toggle="collapse" role="button" href="#details">
           Abstract
       </a>

       <a class="card-link"  target="_blank" href="http://www.openreview.net/pdf/55798e850eb51021c8c65ef727968718b41a91d4.pdf">
           Paper
       </a>
       
       
       <a class="card-link"  target="_blank"  href="http://www.openreview.net/forum?id=HJxK5pEYvr">
           Reviews
       </a>

       <!--  -->
       <!-- <a href="" target="_blank"  class="card-link"> -->
       <!--   Slides -->
       <!-- </a> -->
       <!--  -->

       <!-- </div> -->
     
     <!-- <div class="text-center "> -->
       <a href="" target="_blank"  class="card-link">
           Video
       </a>

       <a href="chat.html?room=channel/paper_channel_HJxK5pEYvr" target="_blank"  class="card-link">
           Chat
       </a>

       


     </div>
   </div>

   </div>

 <div id="details" class="pp-card m-3 collapse">
   <div class="card-body">
     <p class="card-text">
       <div id="abstractExample">
         <span class="font-weight-bold">Abstract:</span>
         Incorporating hierarchical structures like constituency trees has been shown to be effective for various natural language processing (NLP) tasks. However, it is evident that state-of-the-art (SOTA) sequence-based models like the Transformer struggle to encode such structures inherently. On the other hand, dedicated models like the Tree-LSTM, while explicitly modeling hierarchical structures, do not perform as efficiently as the Transformer. In this paper, we attempt to bridge this gap with Hierarchical Accumulation to encode parse tree structures into self-attention at constant time complexity. Our approach outperforms SOTA methods in four IWSLT translation tasks and the WMT&#39;14 English-German task. It also yields improvements over Transformer and Tree-LSTM on three text classification tasks. We further demonstrate that using hierarchical priors can compensate for data shortage, and that our model prefers phrase-level attentions over token-level attentions.
       </div>

     </p>

     <p></p>
     <p class="card-text"><span class="font-weight-bold">Keywords:</span>
       
       <a href="papers.html?filter=keywords&search=Tree" class="text-secondary text-decoration-none">Tree</a>,
       
       <a href="papers.html?filter=keywords&search=Constituency Tree" class="text-secondary text-decoration-none">Constituency Tree</a>,
       
       <a href="papers.html?filter=keywords&search=Hierarchical Accumulation" class="text-secondary text-decoration-none">Hierarchical Accumulation</a>,
       
       <a href="papers.html?filter=keywords&search=Machine Translation" class="text-secondary text-decoration-none">Machine Translation</a>,
       
       <a href="papers.html?filter=keywords&search=NMT" class="text-secondary text-decoration-none">NMT</a>,
       
       <a href="papers.html?filter=keywords&search=WMT" class="text-secondary text-decoration-none">WMT</a>,
       
       <a href="papers.html?filter=keywords&search=IWSLT" class="text-secondary text-decoration-none">IWSLT</a>,
       
       <a href="papers.html?filter=keywords&search=Text Classification" class="text-secondary text-decoration-none">Text Classification</a>,
       
       <a href="papers.html?filter=keywords&search=Sentiment Analysis" class="text-secondary text-decoration-none">Sentiment Analysis</a>
       
     </p>
   </div>
 </div>

</div>

<!-- SlidesLive -->



 <div class="container" style="background-color:white; padding: 0px;">
  <div class="row m-2">
    <div class="col-md-7 col-xs-12 my-auto p-2" >
      <div id="presentation-embed-38915748" class="slp my-auto"></div>
      <script src='https://slideslive.com/embed_presentation.js'></script>
      <script>
        embed = new SlidesLiveEmbed('presentation-embed-38915748', {
        presentationId: '38915748',
        autoPlay: false, // change to true to autoplay the embedded presentation
        verticalEnabled: true,
        verticalWhenWidthLte: 2000,
        allowHiddenControlsWhenPaused: true,
        hideTitle: true
        });
      </script>
    </div>

    <div class="col-md-5 col-xs-12 p-2">
        <div id="gitter" class="slp">
          <center>
             <iframe frameborder="0" src="https://iclr.rocket.chat/channel/paper_channel_HJxK5pEYvr?layout=embedded" height="700px" width="100%" ></iframe>
          </center>
        </div>
      </div>
    </div>
  </div>
</div>

  <!-- Recs -->
  <p></p>


  <div  class="container" style="padding-bottom: 30px; padding-top:30px">
  <center>
    <h2> Similar Papers </h2>
</div>
<p></p>

<div  class="container" >
  <div class="row">

      
      <div class="col-md-4 col-xs-6">
        <div class="pp-card" >
          <div class="pp-card-header" class="text-muted">
            <a href="poster_ByeMPlHKPH.html" class="text-muted">
              <h5 class="card-title" align="center">Lite Transformer with Long-Short Range Attention</h5></a>
            <h6 class="card-subtitle text-muted" align="center">
              
              Zhanghao Wu*,
              
              Zhijian Liu*,
              
              Ji Lin,
              
              Yujun Lin,
              
              Song Han,
              
            </h6>

            <center><img class="cards_img" src="https://iclr.github.io/iclr-images/ByeMPlHKPH.png" width="80%"/></center>

            <!-- <p class="card-text"> Transformer has become ubiquitous in natural language processing (e.g., machine translation, question answering); however, it requires enormous amount of computations to achieve high performance, which makes it not suitable for mobile applications si...</p> -->

          </div>
        </div>
      </div>
      
      <div class="col-md-4 col-xs-6">
        <div class="pp-card" >
          <div class="pp-card-header" class="text-muted">
            <a href="poster_r1eIiCNYwS.html" class="text-muted">
              <h5 class="card-title" align="center">Transformer-XH: Multi-Evidence Reasoning with eXtra Hop Attention</h5></a>
            <h6 class="card-subtitle text-muted" align="center">
              
              Chen Zhao,
              
              Chenyan Xiong,
              
              Corby Rosset,
              
              Xia Song,
              
              Paul Bennett,
              
              Saurabh Tiwary,
              
            </h6>

            <center><img class="cards_img" src="https://iclr.github.io/iclr-images/r1eIiCNYwS.png" width="80%"/></center>

            <!-- <p class="card-text"> We present Transformer-XH, which upgrades Transformer with eXtra Hop attentions to intrinsically model structured texts in a data driven way. </p> -->

          </div>
        </div>
      </div>
      
      <div class="col-md-4 col-xs-6">
        <div class="pp-card" >
          <div class="pp-card-header" class="text-muted">
            <a href="poster_ByxRM0Ntvr.html" class="text-muted">
              <h5 class="card-title" align="center">Are Transformers universal approximators of sequence-to-sequence functions?</h5></a>
            <h6 class="card-subtitle text-muted" align="center">
              
              Chulhee Yun,
              
              Srinadh Bhojanapalli,
              
              Ankit Singh Rawat,
              
              Sashank Reddi,
              
              Sanjiv Kumar,
              
            </h6>

            <center><img class="cards_img" src="https://iclr.github.io/iclr-images/ByxRM0Ntvr.png" width="80%"/></center>

            <!-- <p class="card-text"> We prove that Transformer networks are universal approximators of sequence-to-sequence functions.</p> -->

          </div>
        </div>
      </div>
      
      <div class="col-md-4 col-xs-6">
        <div class="pp-card" >
          <div class="pp-card-header" class="text-muted">
            <a href="poster_ByxY8CNtvr.html" class="text-muted">
              <h5 class="card-title" align="center">Improving Neural Language Generation with Spectrum Control</h5></a>
            <h6 class="card-subtitle text-muted" align="center">
              
              Lingxiao Wang,
              
              Jing Huang,
              
              Kevin Huang,
              
              Ziniu Hu,
              
              Guangtao Wang,
              
              Quanquan Gu,
              
            </h6>

            <center><img class="cards_img" src="https://iclr.github.io/iclr-images/ByxY8CNtvr.png" width="80%"/></center>

            <!-- <p class="card-text"> Recent Transformer-based models such as Transformer-XL and BERT have achieved huge success on various natural language processing tasks. However, contextualized embeddings at the output layer of these powerful models tend to degenerate and occupy an ...</p> -->

          </div>
        </div>
      </div>
      
  </DIV>
</DIV>

</body>