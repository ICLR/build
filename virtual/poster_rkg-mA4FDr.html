<!doctype html>
<html lang="en">
  
  <head>
    <!-- Required meta tags -->
    <meta charset="utf-8">
    <meta name="viewport"
          content="width=device-width, initial-scale=1, shrink-to-fit=no">



    <script src="https://d3js.org/d3.v5.min.js"></script>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/handlebars.js/4.7.3/handlebars.min.js"
            integrity="sha256-/PJBs6QWvXijOFIX04kZpLb6ZtSQckdOIavLWKKOgXU="
            crossorigin="anonymous"></script>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js"
            integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo="
            crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js" integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo" crossorigin="anonymous"></script>


    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/js/bootstrap.min.js" integrity="sha384-wfSDF2E50Y2D1uUdj0O3uMBJnjuUD4Ih7YwaYd1iqfktj0Uod8GCExl3Og8ifwB6" crossorigin="anonymous"></script>


    <script src="static/typeahead.bundle.js"></script>

    <link rel="stylesheet" href="static/typeahead.css">
    <link rel="stylesheet"
          href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css"
          integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T"
          crossorigin="anonymous">

    <title> ICLR: Pre-training Tasks for Embedding-based Large-scale Retrieval </title>
</head>


  <body >

<!-- NAV -->

<script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js" integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/js/bootstrap.min.js" integrity="sha384-wfSDF2E50Y2D1uUdj0O3uMBJnjuUD4Ih7YwaYd1iqfktj0Uod8GCExl3Og8ifwB6" crossorigin="anonymous"></script>
<link href="https://fonts.googleapis.com/css?family=Lato:400,900&display=swap" rel="stylesheet">

<style>
  body{font-family: 'Lato', sans-serif; background-color: rgba(236, 241, 246, 1)}
  .btn-group{background-color: white}
  .btn {background-color: white}
  #main-nav {padding-top:10px; padding-bottom:10px;}
  .card {font-family: 'Cuprum'; box-shadow: 2px 2px 14px 0px rgba(204, 204, 204, 1);}
  .header {font: "Montserrat"; }



  #abstractExample.collapse:not(.show) {
  display: block;
  /* height = lineheight * no of lines to display */
  height: 4.5em;
  overflow: hidden;
  }

  #abstractExample.collapsing {
  height: 4.5em;
  }


#absShow.collapsed:after {
  content: '+ Show More';
}

#absShow:not(.collapsed):after {
  content: '- Show Less';
}
</style>


<nav class="navbar sticky-top navbar-expand-lg navbar-light bg-light mr-auto " id="main-nav" >
  <div class="container">
    <a class="navbar-brand" href="#">
      <img class="logo" style='visibility: ' src="static/ICLR-logo.png"  width="180px" />

    </a>
    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>
    <div class="collapse navbar-collapse text-right flex-grow-1" id="navbarNav">
      <ul class="navbar-nav ml-auto">
        
        <li class="nav-item ">
          <a class="nav-link" href="index.html">Home</a>
        </li>
        
        <li class="nav-item ">
          <a class="nav-link" href="livestream.html">Live</a>
        </li>
        
        <li class="nav-item ">
          <a class="nav-link" href="papers.html">Papers</a>
        </li>
        
        <li class="nav-item ">
          <a class="nav-link" href="paper_vis.html">Vis</a>
        </li>
        
        <li class="nav-item ">
          <a class="nav-link" href="calendar.html">Schedule</a>
        </li>
        
        <li class="nav-item ">
          <a class="nav-link" href="socials.html">Socials</a>
        </li>
        
        <li class="nav-item ">
          <a class="nav-link" href="speakers.html">Speakers</a>
        </li>
        
        <li class="nav-item ">
          <a class="nav-link" href="sponsors.html">Sponsors</a>
        </li>
        
        <li class="nav-item ">
          <a class="nav-link" href="workshops.html">Workshops</a>
        </li>
        
        <li class="nav-item ">
          <a class="nav-link" href="faq.html">FAQ</a>
        </li>
        
      </ul>
    </div>
  </div>
</nav>


<div class="container">

 <!-- Title -->

<div class="card m-3" style="">
  <div class="card-body">
    <h3 class="card-title text-center" style="font-family: 'Cuprum'; font-weight: 700; font-size2.4em; color: #2294e0">
      Pre-training Tasks for Embedding-based Large-scale Retrieval
    </h3>

    <h5 class="card-subtitle mb-2 text-muted text-center">
      
      Wei-Cheng Chang,
      
      Felix X. Yu,
      
      Yin-Wen Chang,
      
      Yiming Yang,
      
      Sanjiv Kumar
      
    </h5>

    <center class="p-3">
      <a class="card-link" data-toggle="collapse" role="button" href="#details">
        <button class="btn btn-outline-secondary">
          Abstract
        </button>
      </a>


      <a class="card-link"  target="_blank" href="http://www.openreview.net/pdf/b90af495b4bef6333a12055e38a1d02a48383f28.pdf">
        <button class="btn btn-outline-secondary">
          Paper
        </button>
      </a>
      <a class="card-link"  target="_blank"  href="http://www.openreview.net/forum?id=rkg-mA4FDr">
        <button class="btn btn-outline-secondary">
          OpenReview
        </button>
      </a>
    <!-- <span><a href="" class="btn btn-secondary">OpenReview</a></span> -->

    <a href="" target="_blank"  class="card-link">
      <button class="btn btn-outline-secondary">
        Code
      </button>
    </a>

    <a href="" target="_blank"  class="card-link">
      <button class="btn btn-outline-secondary">
        Slides
      </button>
    </a>
    </center>

  </div>
</div>

<div id="details" class="card m-3 collapse" style="font-family: 'Cuprum'; box-shadow: 2px 2px 14px 0px rgba(204, 204, 204, 1);">
  <div class="card-body">
    <p class="card-text">
      <div id="abstractExample">
        <span class="font-weight-bold">Abstract:</span>
        We consider the large-scale query-document retrieval problem: given a query (e.g., a question), return the set of relevant documents (e.g., paragraphs containing the answer) from a large document corpus. This problem is often solved in two steps. The retrieval phase first reduces the solution space, returning a subset of candidate documents. The scoring phase then re-ranks the documents. Critically, the retrieval algorithm not only desires high recall but also requires to be highly efficient, returning candidates in time sublinear to the number of documents. Unlike the scoring phase witnessing significant advances recently due to the BERT-style pre-training tasks on cross-attention models, the retrieval phase remains less well studied. Most previous works rely on classic Information Retrieval (IR) methods such as BM-25 (token matching + TF-IDF weights). These models only accept sparse handcrafted features and can not be optimized for different downstream tasks of interest. In this paper, we conduct a comprehensive study on the embedding-based retrieval models. We show that the key ingredient of learning a strong embedding-based Transformer model is the set of pre-training tasks. With adequately designed paragraph-level pre-training tasks, the Transformer models can remarkably improve over the widely-used BM-25 as well as embedding models without Transformers. The paragraph-level pre-training tasks we studied are Inverse Cloze Task (ICT), Body First Selection (BFS), Wiki Link Prediction (WLP), and the combination of all three.
      </div>

    </p>

    <p></p>
    <p class="card-text"><span class="font-weight-bold">Keywords:</span>
      
      <a href="keyword_natural language processing.html" class="text-secondary text-decoration-none">natural language processing</a>,
      
      <a href="keyword_large-scale retrieval.html" class="text-secondary text-decoration-none">large-scale retrieval</a>,
      
      <a href="keyword_unsupervised representation learning.html" class="text-secondary text-decoration-none">unsupervised representation learning</a>,
      
      <a href="keyword_paragraph-level pre-training.html" class="text-secondary text-decoration-none">paragraph-level pre-training</a>,
      
      <a href="keyword_two-tower Transformer models.html" class="text-secondary text-decoration-none">two-tower Transformer models</a>
      
    </p>
  </div>
</div>

<div>
</div>

</div>


<!-- SlidesLive -->
<div id="presentation-embed-38915748" class="container container-sm"></div>
<script src='https://slideslive.com/embed_presentation.js'></script>
<script>
  embed = new SlidesLiveEmbed('presentation-embed-38915748', {
        presentationId: '38915748',
        autoPlay: false, // change to true to autoplay the embedded presentation
        verticalEnabled: true,
        allowHiddenControlsWhenPaused: true,
        zoomRatio: 0.4,
        hideTitle: true
    });
</script>


<!-- Buttons -->
<div class="container" style="padding-bottom: 30px; padding-top:30px">
  <center>
    <h2> Paper Discussion       </h2>
    <!-- <span><a class="btn btn-secondary" href="https://gitter.im/iclr/posterrkg-mA4FDr/">Chat</a></span> -->
  </center>
  <p></p>

  <!-- Gitter -->
  <!-- <div id='discourse-comments'></div> -->


<!-- <script type="text/javascript"> -->
<!--   DiscourseEmbed = { discourseUrl: 'https://iclr.trydiscourse.com/', -->
<!--                      topicId: 20}; -->

<!--   (function() { -->
<!--     var d = document.createElement('script'); d.type = 'text/javascript'; d.async = true; -->
<!--     d.src = DiscourseEmbed.discourseUrl + 'javascripts/embed.js'; -->
<!--     (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(d); -->
<!--   })(); -->
<!-- </script> -->


  <div id="gitter" class="gitter container" height="600px">
    <center>
      <div class="border">

        <center> <iframe frameborder="0" src="https://iclr.rocket.chat/channel/paper_channel_rkg-mA4FDr?layout=embedded" width="900px" height="400px"></iframe> </center>
      </div>
    </center>
  </div>
</div>


  <!-- Recs -->
  <p></p>


  <div  class="container" style="padding-bottom: 30px; padding-top:30px">
  <center>
    <h2> Similar Papers </h2>
</div>
<p></p>

<div  class="container" >
  <div class="row">
    <div class="card-deck">

      

  <!-- <div class="col-sm-4" style="padding-bottom: 10px"> -->
  <div class="card" >
    <div class="card-header">
        <a href="poster_SkxgnnNFvH.html" class="text-dark"><h5 class="card-title">Poly-encoders: Architectures and Pre-training Strategies for Fast and Accurate Multi-sentence Scoring</h5></a>

    </div>
      <div class="card-body">
        <p class="card-text"> The use of deep pre-trained transformers has led to remarkable progress in a number of applications (Devlin et al., 2018). For tasks that make pairwise comparisons between sequences, matching a given input with a corresponding label, two approaches a...</p>
      </div>

      <div class="card-footer">
      <center>
        <a href="poster_SkxgnnNFvH.html" class="btn btn-primary">Visit</a>
        </center>

      </div>
      <!-- </div> -->

  </div>
  

  <!-- <div class="col-sm-4" style="padding-bottom: 10px"> -->
  <div class="card" >
    <div class="card-header">
        <a href="poster_HJlWWJSFDH.html" class="text-dark"><h5 class="card-title">Strategies for Pre-training Graph Neural Networks</h5></a>

    </div>
      <div class="card-body">
        <p class="card-text"> We develop a strategy for pre-training Graph Neural Networks (GNNs) and systematically study its effectiveness on multiple datasets, GNN architectures, and diverse downstream tasks.</p>
      </div>

      <div class="card-footer">
      <center>
        <a href="poster_HJlWWJSFDH.html" class="btn btn-primary">Visit</a>
        </center>

      </div>
      <!-- </div> -->

  </div>
  

  <!-- <div class="col-sm-4" style="padding-bottom: 10px"> -->
  <div class="card" >
    <div class="card-header">
        <a href="poster_BJgQ4lSFPH.html" class="text-dark"><h5 class="card-title">StructBERT: Incorporating Language Structures into Pre-training for Deep Language Understanding</h5></a>

    </div>
      <div class="card-body">
        <p class="card-text"> Recently, the pre-trained language model, BERT (and its robustly optimized version RoBERTa), has attracted a lot of attention in natural language understanding (NLU), and achieved state-of-the-art accuracy in various NLU tasks, such as sentiment clas...</p>
      </div>

      <div class="card-footer">
      <center>
        <a href="poster_BJgQ4lSFPH.html" class="btn btn-primary">Visit</a>
        </center>

      </div>
      <!-- </div> -->

  </div>
  

  <!-- <div class="col-sm-4" style="padding-bottom: 10px"> -->
  <div class="card" >
    <div class="card-header">
        <a href="poster_SJgdnAVKDH.html" class="text-dark"><h5 class="card-title">Revisiting Self-Training for Neural Sequence Generation</h5></a>

    </div>
      <div class="card-body">
        <p class="card-text"> We revisit self-training as a semi-supervised learning method for neural sequence generation problem, and show that self-training can be quite successful with injected noise.</p>
      </div>

      <div class="card-footer">
      <center>
        <a href="poster_SJgdnAVKDH.html" class="btn btn-primary">Visit</a>
        </center>

      </div>
      <!-- </div> -->

  </div>
  
    </DIV>
          </DIV>
      </DIV>

</body>