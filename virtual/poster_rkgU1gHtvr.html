<!doctype html>
<html lang="en">
  
  <head>
    <!-- Required meta tags -->
    <meta charset="utf-8">
    <meta name="viewport"
          content="width=device-width, initial-scale=1, shrink-to-fit=no">

    <link rel="stylesheet" href="static/css/main.css">

    <script src="https://d3js.org/d3.v5.min.js"></script>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/handlebars.js/4.7.3/handlebars.min.js"
            integrity="sha256-/PJBs6QWvXijOFIX04kZpLb6ZtSQckdOIavLWKKOgXU="
            crossorigin="anonymous"></script>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js"
            integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo="
            crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js" integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo" crossorigin="anonymous"></script>


    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/js/bootstrap.min.js" integrity="sha384-wfSDF2E50Y2D1uUdj0O3uMBJnjuUD4Ih7YwaYd1iqfktj0Uod8GCExl3Og8ifwB6" crossorigin="anonymous"></script>


    <script src="static/js/typeahead.bundle.js"></script>

    <link rel="stylesheet" href="static/css/typeahead.css">
    <link rel="stylesheet"
          href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css"
          integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T"
          crossorigin="anonymous">

    <title> ICLR: Infinite-horizon Off-Policy Policy Evaluation with Multiple Behavior Policies </title>
</head>


  <body >

<!-- NAV -->

<script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js" integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/js/bootstrap.min.js" integrity="sha384-wfSDF2E50Y2D1uUdj0O3uMBJnjuUD4Ih7YwaYd1iqfktj0Uod8GCExl3Og8ifwB6" crossorigin="anonymous"></script>
<link href="https://fonts.googleapis.com/css?family=Lato:400,900&display=swap" rel="stylesheet">
<link href="https://fonts.googleapis.com/css?family=Exo" rel='stylesheet'>
<link href="https://fonts.googleapis.com/css?family=Cuprum" rel='stylesheet'>
<nav class="navbar sticky-top navbar-expand-lg navbar-light bg-light mr-auto " id="main-nav" >
  <div class="container">
    <a class="navbar-brand" href="#">
      <img class="logo" style='visibility: ' src="static/images/ICLR-logo.png"  width="180px" />

    </a>
    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>
    <div class="collapse navbar-collapse text-right flex-grow-1" id="navbarNav">
      <ul class="navbar-nav ml-auto">
        
        <li class="nav-item ">
          <a class="nav-link" href="index.html">Home</a>
        </li>
        
        <li class="nav-item ">
          <a class="nav-link" href="papers.html">Browse</a>
        </li>
        
        <li class="nav-item ">
          <a class="nav-link" href="calendar.html">Schedule</a>
        </li>
        
        <li class="nav-item ">
          <a class="nav-link" href="events.html">Events</a>
        </li>
        
        <li class="nav-item ">
          <a class="nav-link" href="sponsors.html">Booths</a>
        </li>
        
        <li class="nav-item ">
          <a class="nav-link" href="paper_vis.html">Extras</a>
        </li>
        
        <li class="nav-item ">
          <a class="nav-link" href="chat.html">Chat</a>
        </li>
        
        <li class="nav-item ">
          <a class="nav-link" href="about.html">About</a>
        </li>
        
        <li class="nav-item ">
          <a class="nav-link" href="faq.html">Help</a>
        </li>
        
      </ul>
    </div>
  </div>
</nav>


<div class="container">

 <!-- Title -->

 <div class="pp-card m-3" style="">
   <div class="card-header">
     <h2 class="card-title main-title text-center" style="">
       Infinite-horizon Off-Policy Policy Evaluation with Multiple Behavior Policies
     </h2>

     <h3 class="card-subtitle mb-2 text-muted text-center">
       
       Xinyun Chen,
       
       Lu Wang,
       
       Yizhe Hang,
       
       Heng Ge,
       
       Hongyuan Zha
       
     </h3>

     <div class="text-center p-3">
       <a class="card-link" data-toggle="collapse" role="button" href="#details">
           Abstract
       </a>

       <a class="card-link"  target="_blank" href="http://www.openreview.net/pdf/7efb165a914c892ca47ef91f33ab01dc593aa66a.pdf">
           Paper
       </a>
       <a class="card-link"  target="_blank"  href="http://www.openreview.net/forum?id=rkgU1gHtvr">
           OpenReview
       </a>

     <!-- </div> -->
     
     <!-- <div class="text-center "> -->
       <a href="" target="_blank"  class="card-link">
           Zoom
       </a>

       <a href="https://iclr.rocket.chat/channel/paper_channel_rkgU1gHtvr" target="_blank"  class="card-link">
           Chat
       </a>

       

       
     </div>
   </div>

   </div>

 <div id="details" class="pp-card m-3 collapse">
   <div class="card-body">
     <p class="card-text">
       <div id="abstractExample">
         <span class="font-weight-bold">Abstract:</span>
         We consider off-policy policy evaluation when the trajectory data are generated by multiple behavior policies. Recent work has shown the key role played by the state or state-action stationary distribution corrections in the infinite horizon context for off-policy policy evaluation. We propose estimated mixture policy (EMP), a novel class of partially policy-agnostic methods to accurately estimate those quantities. With careful analysis, we show that EMP gives rise to estimates with reduced variance for estimating the state stationary distribution correction while it also offers a useful induction bias for estimating the state-action stationary distribution correction. In extensive experiments with both continuous and discrete environments, we demonstrate that our algorithm offers significantly improved accuracy compared to the state-of-the-art methods.
       </div>

     </p>

     <p></p>
     <p class="card-text"><span class="font-weight-bold">Keywords:</span>
       
       <a href="papers.html?filter=keywords&search=off-policy policy evaluation" class="text-secondary text-decoration-none">off-policy policy evaluation</a>,
       
       <a href="papers.html?filter=keywords&search=multiple importance sampling" class="text-secondary text-decoration-none">multiple importance sampling</a>,
       
       <a href="papers.html?filter=keywords&search=kernel method" class="text-secondary text-decoration-none">kernel method</a>,
       
       <a href="papers.html?filter=keywords&search=variance reduction" class="text-secondary text-decoration-none">variance reduction</a>
       
     </p>
   </div>
 </div>

</div>

<!-- SlidesLive -->



 <div class="container" style="background-color:white; padding: 0px;">
  <div class="row m-2">
    <div class="col-md-7 col-xs-12 my-auto p-2" >
      <div id="presentation-embed-38915748" class="slp my-auto"></div>
      <script src='https://slideslive.com/embed_presentation.js'></script>
      <script>
        embed = new SlidesLiveEmbed('presentation-embed-38915748', {
        presentationId: '38915748',
        autoPlay: false, // change to true to autoplay the embedded presentation
        verticalEnabled: true,
        verticalWhenWidthLte: 2000,
        allowHiddenControlsWhenPaused: true,
        hideTitle: true
        });
      </script>
    </div>

    <div class="col-md-5 col-xs-12 p-2">
        <div id="gitter" class="slp">
          <center>
             <iframe frameborder="0" src="https://iclr.rocket.chat/channel/paper_channel_rkgU1gHtvr?layout=embedded" height="700px" width="100%" ></iframe>
          </center>
        </div>
      </div>
    </div>
  </div>
</div>

  <!-- Recs -->
  <p></p>


  <div  class="container" style="padding-bottom: 30px; padding-top:30px">
  <center>
    <h2> Similar Papers </h2>
</div>
<p></p>

<div  class="container" >
  <div class="row">

      
      <div class="col-md-4 col-xs-6">
        <div class="pp-card" >
          <div class="pp-card-header" class="text-muted">
            <a href="poster_HkxlcnVFwB.html" class="text-muted">
              <h5 class="card-title" align="center">GenDICE: Generalized Offline Estimation of Stationary Values</h5></a>
            <h6 class="card-subtitle text-muted" align="center">
              
              Ruiyi Zhang*,
              
              Bo Dai*,
              
              Lihong Li,
              
              Dale Schuurmans,
              
            </h6>

            <center><img class="cards_img" src="https://iclr.github.io/iclr-images/HkxlcnVFwB.png" width="80%"/></center>

            <!-- <p class="card-text"> In this paper, we proposed a novel algorithm, GenDICE, for general stationary distribution correction estimation, which can handle both discounted and average off-policy evaluation on multiple behavior-agnostic samples.</p> -->

          </div>
        </div>
      </div>
      
      <div class="col-md-4 col-xs-6">
        <div class="pp-card" >
          <div class="pp-card-header" class="text-muted">
            <a href="poster_S1glGANtDr.html" class="text-muted">
              <h5 class="card-title" align="center">Doubly Robust Bias Reduction in Infinite Horizon Off-Policy Estimation</h5></a>
            <h6 class="card-subtitle text-muted" align="center">
              
              Ziyang Tang*,
              
              Yihao Feng*,
              
              Lihong Li,
              
              Dengyong Zhou,
              
              Qiang Liu,
              
            </h6>

            <center><img class="cards_img" src="https://iclr.github.io/iclr-images/S1glGANtDr.png" width="80%"/></center>

            <!-- <p class="card-text"> We develop a new doubly robust estimator based on the infinite horizon density ratio and off policy value estimation.</p> -->

          </div>
        </div>
      </div>
      
      <div class="col-md-4 col-xs-6">
        <div class="pp-card" >
          <div class="pp-card-header" class="text-muted">
            <a href="poster_rJld3hEYvS.html" class="text-muted">
              <h5 class="card-title" align="center">Ranking Policy Gradient</h5></a>
            <h6 class="card-subtitle text-muted" align="center">
              
              Kaixiang Lin,
              
              Jiayu Zhou,
              
            </h6>

            <center><img class="cards_img" src="https://iclr.github.io/iclr-images/rJld3hEYvS.png" width="80%"/></center>

            <!-- <p class="card-text"> We propose ranking policy gradient that learns the optimal rank of actions to maximize return. We propose a general off-policy learning framework with the properties of optimality preserving, variance reduction, and sample-efficiency.</p> -->

          </div>
        </div>
      </div>
      
      <div class="col-md-4 col-xs-6">
        <div class="pp-card" >
          <div class="pp-card-header" class="text-muted">
            <a href="poster_rJeINp4KwH.html" class="text-muted">
              <h5 class="card-title" align="center">Population-Guided Parallel Policy Search for Reinforcement Learning</h5></a>
            <h6 class="card-subtitle text-muted" align="center">
              
              Whiyoung Jung,
              
              Giseung Park,
              
              Youngchul Sung,
              
            </h6>

            <center><img class="cards_img" src="https://iclr.github.io/iclr-images/rJeINp4KwH.png" width="80%"/></center>

            <!-- <p class="card-text"> In this paper, a new population-guided parallel learning scheme is proposed to enhance the performance of off-policy reinforcement learning (RL). In the proposed scheme, multiple identical learners with their own value-functions and policies share a ...</p> -->

          </div>
        </div>
      </div>
      
  </DIV>
</DIV>

</body>