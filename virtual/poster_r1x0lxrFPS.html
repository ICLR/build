<!doctype html>
<html lang="en">
  
  <head>
    <!-- Required meta tags -->
    <meta charset="utf-8">
    <meta name="viewport"
          content="width=device-width, initial-scale=1, shrink-to-fit=no">

    <link rel="stylesheet" href="static/css/main.css">

    <script src="https://d3js.org/d3.v5.min.js"></script>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/handlebars.js/4.7.3/handlebars.min.js"
            integrity="sha256-/PJBs6QWvXijOFIX04kZpLb6ZtSQckdOIavLWKKOgXU="
            crossorigin="anonymous"></script>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js"
            integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo="
            crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js" integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo" crossorigin="anonymous"></script>


    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/js/bootstrap.min.js" integrity="sha384-wfSDF2E50Y2D1uUdj0O3uMBJnjuUD4Ih7YwaYd1iqfktj0Uod8GCExl3Og8ifwB6" crossorigin="anonymous"></script>


    <script src="static/js/typeahead.bundle.js"></script>

    <link rel="stylesheet" href="static/css/typeahead.css">
    <link rel="stylesheet"
          href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css"
          integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T"
          crossorigin="anonymous">

    <title> ICLR: BinaryDuo: Reducing Gradient Mismatch in Binary Activation Network by Coupling Binary Activations </title>
</head>


  <body >

<!-- NAV -->

<script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js" integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/js/bootstrap.min.js" integrity="sha384-wfSDF2E50Y2D1uUdj0O3uMBJnjuUD4Ih7YwaYd1iqfktj0Uod8GCExl3Og8ifwB6" crossorigin="anonymous"></script>
<link href="https://fonts.googleapis.com/css?family=Lato:400,900&display=swap" rel="stylesheet">
<link href="https://fonts.googleapis.com/css?family=Exo" rel='stylesheet'>
<link href="https://fonts.googleapis.com/css?family=Cuprum" rel='stylesheet'>
<nav class="navbar sticky-top navbar-expand-lg navbar-light bg-light mr-auto " id="main-nav" >
  <div class="container">
    <a class="navbar-brand" href="index.html">
      <img class="logo" style='visibility: ' src="static/images/ICLR-logo.png"  width="180px" />

    </a>
    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>
    <div class="collapse navbar-collapse text-right flex-grow-1" id="navbarNav">
      <ul class="navbar-nav ml-auto">
        
        <li class="nav-item ">
          <a class="nav-link"    href="index.html">Home</a>
        </li>
        
        <li class="nav-item ">
          <a class="nav-link"    href="calendar.html">Schedule</a>
        </li>
        
        <li class="nav-item ">
          <a class="nav-link"    href="workshops.html">Workshops</a>
        </li>
        
        <li class="nav-item ">
          <a class="nav-link"    href="papers.html">Papers</a>
        </li>
        
        <li class="nav-item ">
          <a class="nav-link" target="_blank"   href="https://iclr.6connex.com/event/VirtualEvent/">Sponsor Hall</a>
        </li>
        
        <li class="nav-item ">
          <a class="nav-link"    href="socials.html">Socials</a>
        </li>
        
        <li class="nav-item ">
          <a class="nav-link"    href="chat.html">Chat</a>
        </li>
        
        <li class="nav-item ">
          <a class="nav-link"    href="about.html">Help</a>
        </li>
        
      </ul>
    </div>
  </div>
</nav>


<div class="container">

 <!-- Title -->

 <div class="pp-card m-3" style="">
   <div class="card-header">
     <h2 class="card-title main-title text-center" style="">
       BinaryDuo: Reducing Gradient Mismatch in Binary Activation Network by Coupling Binary Activations
     </h2>

     <h3 class="card-subtitle mb-2 text-muted text-center">
       
       Hyungjun Kim,
       
       Kyungsu Kim,
       
       Jinseok Kim,
       
       Jae-Joon Kim
       
     </h3>

     <div class="text-center p-3">
       <a class="card-link" data-toggle="collapse" role="button" href="#details">
           Abstract
       </a>

       <a class="card-link"  target="_blank" href="http://www.openreview.net/pdf/d2650d092434446c1f01a83f6ccdebefd03bfdd9.pdf">
           Paper
       </a>
       
       <a href="https://github.com/Hyungjun-K1m/BinaryDuo" target="_blank"  class="card-link">
           Code
       </a>
       
       
       <a class="card-link"  target="_blank"  href="http://www.openreview.net/forum?id=r1x0lxrFPS">
           Reviews
       </a>

       <!--  -->
       <!-- <a href="" target="_blank"  class="card-link"> -->
       <!--   Slides -->
       <!-- </a> -->
       <!--  -->

       <!-- </div> -->
     
     <!-- <div class="text-center "> -->
       <a href="" target="_blank"  class="card-link">
           Video
       </a>

       <a href="chat.html?room=channel/paper_channel_r1x0lxrFPS" target="_blank"  class="card-link">
           Chat
       </a>

       


     </div>
   </div>

   </div>

 <div id="details" class="pp-card m-3 collapse">
   <div class="card-body">
     <p class="card-text">
       <div id="abstractExample">
         <span class="font-weight-bold">Abstract:</span>
         Binary Neural Networks (BNNs) have been garnering interest thanks to their compute cost reduction and memory savings. However, BNNs suffer from performance degradation mainly due to the gradient mismatch caused by binarizing activations. Previous works tried to address the gradient mismatch problem by reducing the discrepancy between activation functions used at forward pass and its differentiable approximation used at backward pass, which is an indirect measure. In this work, we use the gradient of smoothed loss function to better estimate the gradient mismatch in quantized neural network. Analysis using the gradient mismatch estimator indicates that using higher precision for activation is more effective than modifying the differentiable approximation of activation function. Based on the observation, we propose a new training scheme for binary activation networks called BinaryDuo in which two binary activations are coupled into a ternary activation during training. Experimental results show that BinaryDuo outperforms state-of-the-art BNNs on various benchmarks with the same amount of parameters and computing cost.
       </div>

     </p>

     <p></p>
     <p class="card-text"><span class="font-weight-bold">Keywords:</span>
       
     </p>
   </div>
 </div>

</div>

<!-- SlidesLive -->



 <div class="container" style="background-color:white; padding: 0px;">
  <div class="row m-2">
    <div class="col-md-7 col-xs-12 my-auto p-2" >
      <div id="presentation-embed-38915748" class="slp my-auto"></div>
      <script src='https://slideslive.com/embed_presentation.js'></script>
      <script>
        embed = new SlidesLiveEmbed('presentation-embed-38915748', {
        presentationId: '38915748',
        autoPlay: false, // change to true to autoplay the embedded presentation
        verticalEnabled: true,
        verticalWhenWidthLte: 2000,
        allowHiddenControlsWhenPaused: true,
        hideTitle: true
        });
      </script>
    </div>

    <div class="col-md-5 col-xs-12 p-2">
        <div id="gitter" class="slp">
          <center>
             <iframe frameborder="0" src="https://iclr.rocket.chat/channel/paper_channel_r1x0lxrFPS?layout=embedded" height="700px" width="100%" ></iframe>
          </center>
        </div>
      </div>
    </div>
  </div>
</div>

  <!-- Recs -->
  <p></p>


  <div  class="container" style="padding-bottom: 30px; padding-top:30px">
  <center>
    <h2> Similar Papers </h2>
</div>
<p></p>

<div  class="container" >
  <div class="row">

      
      <div class="col-md-4 col-xs-6">
        <div class="pp-card" >
          <div class="pp-card-header" class="text-muted">
            <a href="poster_BJlBSkHtDS.html" class="text-muted">
              <h5 class="card-title" align="center">Pad√© Activation Units: End-to-end Learning of Flexible Activation Functions in Deep Networks</h5></a>
            <h6 class="card-subtitle text-muted" align="center">
              
              Alejandro Molina,
              
              Patrick Schramowski,
              
              Kristian Kersting,
              
            </h6>

            <center><img class="cards_img" src="https://iclr.github.io/iclr-images/BJlBSkHtDS.png" width="80%"/></center>

            <!-- <p class="card-text"> We introduce PAU, a new learnable activation function for neural networks. They free the network designers from the activation selection process and increase the test prediction accuracy.</p> -->

          </div>
        </div>
      </div>
      
      <div class="col-md-4 col-xs-6">
        <div class="pp-card" >
          <div class="pp-card-header" class="text-muted">
            <a href="poster_BJg4NgBKvH.html" class="text-muted">
              <h5 class="card-title" align="center">Training binary neural networks with real-to-binary convolutions</h5></a>
            <h6 class="card-subtitle text-muted" align="center">
              
              Brais Martinez,
              
              Jing Yang,
              
              Adrian Bulat,
              
              Georgios Tzimiropoulos,
              
            </h6>

            <center><img class="cards_img" src="https://iclr.github.io/iclr-images/BJg4NgBKvH.png" width="80%"/></center>

            <!-- <p class="card-text"> This paper shows how to train binary networks to within a few percent points (~3-5%) of the full precision counterpart. We first show how to build a strong baseline, which already achieves state-of-the-art accuracy, by combining recently proposed adv...</p> -->

          </div>
        </div>
      </div>
      
      <div class="col-md-4 col-xs-6">
        <div class="pp-card" >
          <div class="pp-card-header" class="text-muted">
            <a href="poster_rkgO66VKDS.html" class="text-muted">
              <h5 class="card-title" align="center">LEARNED STEP SIZE QUANTIZATION</h5></a>
            <h6 class="card-subtitle text-muted" align="center">
              
              Steven K. Esser,
              
              Jeffrey L. McKinstry,
              
              Deepika Bablani,
              
              Rathinakumar Appuswamy,
              
              Dharmendra S. Modha,
              
            </h6>

            <center><img class="cards_img" src="https://iclr.github.io/iclr-images/rkgO66VKDS.png" width="80%"/></center>

            <!-- <p class="card-text"> A method for learning quantization configuration for low precision networks that achieves state of the art performance for quantized networks.</p> -->

          </div>
        </div>
      </div>
      
      <div class="col-md-4 col-xs-6">
        <div class="pp-card" >
          <div class="pp-card-header" class="text-muted">
            <a href="poster_Hyx0slrFvH.html" class="text-muted">
              <h5 class="card-title" align="center">Mixed Precision DNNs: All you need is a good parametrization</h5></a>
            <h6 class="card-subtitle text-muted" align="center">
              
              Stefan Uhlich,
              
              Lukas Mauch,
              
              Fabien Cardinaux,
              
              Kazuki Yoshiyama,
              
              Javier Alonso Garcia,
              
              Stephen Tiedemann,
              
              Thomas Kemp,
              
              Akira Nakamura,
              
            </h6>

            <center><img class="cards_img" src="https://iclr.github.io/iclr-images/Hyx0slrFvH.png" width="80%"/></center>

            <!-- <p class="card-text"> Efficient deep neural network (DNN) inference on mobile or embedded devices typically involves quantization of the network parameters and activations. In particular, mixed precision networks achieve better performance than networks with homogeneous b...</p> -->

          </div>
        </div>
      </div>
      
  </DIV>
</DIV>

</body>