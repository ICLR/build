<!doctype html>
<html lang="en">
  
  <head>
    <!-- Required meta tags -->
    <meta charset="utf-8">
    <meta name="viewport"
          content="width=device-width, initial-scale=1, shrink-to-fit=no">

    <link rel="stylesheet" href="static/css/main.css">

    <script src="https://d3js.org/d3.v5.min.js"></script>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/handlebars.js/4.7.3/handlebars.min.js"
            integrity="sha256-/PJBs6QWvXijOFIX04kZpLb6ZtSQckdOIavLWKKOgXU="
            crossorigin="anonymous"></script>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js"
            integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo="
            crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js" integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo" crossorigin="anonymous"></script>


    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/js/bootstrap.min.js" integrity="sha384-wfSDF2E50Y2D1uUdj0O3uMBJnjuUD4Ih7YwaYd1iqfktj0Uod8GCExl3Og8ifwB6" crossorigin="anonymous"></script>


    <script src="static/js/typeahead.bundle.js"></script>

    <link rel="stylesheet" href="static/css/typeahead.css">
    <link rel="stylesheet"
          href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css"
          integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T"
          crossorigin="anonymous">

    <title> ICLR: U-GAT-IT: Unsupervised Generative Attentional Networks with Adaptive Layer-Instance Normalization for Image-to-Image Translation </title>
</head>


  <body >

<!-- NAV -->

<script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js" integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/js/bootstrap.min.js" integrity="sha384-wfSDF2E50Y2D1uUdj0O3uMBJnjuUD4Ih7YwaYd1iqfktj0Uod8GCExl3Og8ifwB6" crossorigin="anonymous"></script>
<link href="https://fonts.googleapis.com/css?family=Lato:400,900&display=swap" rel="stylesheet">
<link href="https://fonts.googleapis.com/css?family=Exo" rel='stylesheet'>
<link href="https://fonts.googleapis.com/css?family=Cuprum" rel='stylesheet'>
<nav class="navbar sticky-top navbar-expand-lg navbar-light bg-light mr-auto " id="main-nav" >
  <div class="container">
    <a class="navbar-brand" href="#">
      <img class="logo" style='visibility: ' src="static/images/ICLR-logo.png"  width="180px" />

    </a>
    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>
    <div class="collapse navbar-collapse text-right flex-grow-1" id="navbarNav">
      <ul class="navbar-nav ml-auto">
        
        <li class="nav-item ">
          <a class="nav-link" href="index.html">Home</a>
        </li>
        
        <li class="nav-item ">
          <a class="nav-link" href="papers.html">Browse</a>
        </li>
        
        <li class="nav-item ">
          <a class="nav-link" href="calendar.html">Schedule</a>
        </li>
        
        <li class="nav-item ">
          <a class="nav-link" href="events.html">Events</a>
        </li>
        
        <li class="nav-item ">
          <a class="nav-link" href="sponsors.html">Booths</a>
        </li>
        
        <li class="nav-item ">
          <a class="nav-link" href="paper_vis.html">Extras</a>
        </li>
        
        <li class="nav-item ">
          <a class="nav-link" href="chat.html">Chat</a>
        </li>
        
        <li class="nav-item ">
          <a class="nav-link" href="about.html">About</a>
        </li>
        
        <li class="nav-item ">
          <a class="nav-link" href="faq.html">Help</a>
        </li>
        
      </ul>
    </div>
  </div>
</nav>


<div class="container">

 <!-- Title -->

 <div class="pp-card m-3" style="">
   <div class="card-header">
     <h2 class="card-title main-title text-center" style="">
       U-GAT-IT: Unsupervised Generative Attentional Networks with Adaptive Layer-Instance Normalization for Image-to-Image Translation
     </h2>

     <h3 class="card-subtitle mb-2 text-muted text-center">
       
       Junho Kim,
       
       Minjae Kim,
       
       Hyeonwoo Kang,
       
       Kwang Hee Lee
       
     </h3>

     <div class="text-center p-3">
       <a class="card-link" data-toggle="collapse" role="button" href="#details">
           Abstract
       </a>

       <a class="card-link"  target="_blank" href="http://www.openreview.net/pdf/65dec7deec5fb3a5b6c95d3e737070a669ca3033.pdf">
           Paper
       </a>
       <a class="card-link"  target="_blank"  href="http://www.openreview.net/forum?id=BJlZ5ySKPH">
           OpenReview
       </a>

     <!-- </div> -->
     
     <!-- <div class="text-center "> -->
       <a href="" target="_blank"  class="card-link">
           Zoom
       </a>

       <a href="https://iclr.rocket.chat/channel/paper_channel_BJlZ5ySKPH" target="_blank"  class="card-link">
           Chat
       </a>

       
       <a href="https://github.com/taki0112/UGATIT" target="_blank"  class="card-link">
           Code
       </a>
       

       
     </div>
   </div>

   </div>

 <div id="details" class="pp-card m-3 collapse">
   <div class="card-body">
     <p class="card-text">
       <div id="abstractExample">
         <span class="font-weight-bold">Abstract:</span>
         We propose a novel method for unsupervised image-to-image translation, which incorporates a new attention module and a new learnable normalization function in an end-to-end manner. The attention module guides our model to focus on more important regions distinguishing between source and target domains based on the attention map obtained by the auxiliary classifier. Unlike previous attention-based method which cannot handle the geometric changes between domains, our model can translate both images requiring holistic changes and images requiring large shape changes. Moreover, our new AdaLIN (Adaptive Layer-Instance Normalization) function helps our attention-guided model to flexibly control the amount of change in shape and texture by learned parameters depending on datasets. Experimental results show the superiority of the proposed method compared to the existing state-of-the-art models with a fixed network architecture and hyper-parameters. 
       </div>

     </p>

     <p></p>
     <p class="card-text"><span class="font-weight-bold">Keywords:</span>
       
       <a href="papers.html?filter=keywords&search=Image-to-Image Translation" class="text-secondary text-decoration-none">Image-to-Image Translation</a>,
       
       <a href="papers.html?filter=keywords&search=Generative Attentional Networks" class="text-secondary text-decoration-none">Generative Attentional Networks</a>,
       
       <a href="papers.html?filter=keywords&search=Adaptive Layer-Instance Normalization" class="text-secondary text-decoration-none">Adaptive Layer-Instance Normalization</a>
       
     </p>
   </div>
 </div>

</div>

<!-- SlidesLive -->



 <div class="container" style="background-color:white; padding: 0px;">
  <div class="row m-2">
    <div class="col-md-7 col-xs-12 my-auto p-2" >
      <div id="presentation-embed-38915748" class="slp my-auto"></div>
      <script src='https://slideslive.com/embed_presentation.js'></script>
      <script>
        embed = new SlidesLiveEmbed('presentation-embed-38915748', {
        presentationId: '38915748',
        autoPlay: false, // change to true to autoplay the embedded presentation
        verticalEnabled: true,
        verticalWhenWidthLte: 2000,
        allowHiddenControlsWhenPaused: true,
        hideTitle: true
        });
      </script>
    </div>

    <div class="col-md-5 col-xs-12 p-2">
        <div id="gitter" class="slp">
          <center>
             <iframe frameborder="0" src="https://iclr.rocket.chat/channel/paper_channel_BJlZ5ySKPH?layout=embedded" height="700px" width="100%" ></iframe>
          </center>
        </div>
      </div>
    </div>
  </div>
</div>

  <!-- Recs -->
  <p></p>


  <div  class="container" style="padding-bottom: 30px; padding-top:30px">
  <center>
    <h2> Similar Papers </h2>
</div>
<p></p>

<div  class="container" >
  <div class="row">

      
      <div class="col-md-4 col-xs-6">
        <div class="pp-card" >
          <div class="pp-card-header" class="text-muted">
            <a href="poster_Hyg96gBKPS.html" class="text-muted">
              <h5 class="card-title" align="center">Monotonic Multihead Attention</h5></a>
            <h6 class="card-subtitle text-muted" align="center">
              
              Xutai Ma,
              
              Juan Miguel Pino,
              
              James Cross,
              
              Liezl Puzon,
              
              Jiatao Gu,
              
            </h6>

            <center><img class="cards_img" src="https://iclr.github.io/iclr-images/Hyg96gBKPS.png" width="80%"/></center>

            <!-- <p class="card-text"> Make the transformer streamable with monotonic attention.</p> -->

          </div>
        </div>
      </div>
      
      <div class="col-md-4 col-xs-6">
        <div class="pp-card" >
          <div class="pp-card-header" class="text-muted">
            <a href="poster_SJl5Np4tPr.html" class="text-muted">
              <h5 class="card-title" align="center">Cross-Domain Few-Shot Classification via Learned Feature-Wise Transformation</h5></a>
            <h6 class="card-subtitle text-muted" align="center">
              
              Hung-Yu Tseng,
              
              Hsin-Ying Lee,
              
              Jia-Bin Huang,
              
              Ming-Hsuan Yang,
              
            </h6>

            <center><img class="cards_img" src="https://iclr.github.io/iclr-images/SJl5Np4tPr.png" width="80%"/></center>

            <!-- <p class="card-text"> Few-shot classification aims to recognize novel categories with only few labeled images in each class. Existing metric-based few-shot classification algorithms predict categories by comparing the feature embeddings of query images with those from a f...</p> -->

          </div>
        </div>
      </div>
      
      <div class="col-md-4 col-xs-6">
        <div class="pp-card" >
          <div class="pp-card-header" class="text-muted">
            <a href="poster_BJe-91BtvH.html" class="text-muted">
              <h5 class="card-title" align="center">Masked Based Unsupervised Content Transfer</h5></a>
            <h6 class="card-subtitle text-muted" align="center">
              
              Ron Mokady,
              
              Sagie Benaim,
              
              Lior Wolf,
              
              Amit Bermano,
              
            </h6>

            <center><img class="cards_img" src="https://iclr.github.io/iclr-images/BJe-91BtvH.png" width="80%"/></center>

            <!-- <p class="card-text"> We consider the problem of translating, in an unsupervised manner, between two domains where one contains some additional information compared to the other. The proposed method  disentangles the common and separate parts of these domains and, through...</p> -->

          </div>
        </div>
      </div>
      
      <div class="col-md-4 col-xs-6">
        <div class="pp-card" >
          <div class="pp-card-header" class="text-muted">
            <a href="poster_B1gF56VYPH.html" class="text-muted">
              <h5 class="card-title" align="center">Deep 3D Pan via Local adaptive &#34;t-shaped&#34; convolutions with global and local adaptive dilations</h5></a>
            <h6 class="card-subtitle text-muted" align="center">
              
              Juan Luis Gonzalez Bello,
              
              Munchurl Kim,
              
            </h6>

            <center><img class="cards_img" src="https://iclr.github.io/iclr-images/B1gF56VYPH.png" width="80%"/></center>

            <!-- <p class="card-text"> Novel architecture for stereoscopic view synthesis at arbitrary camera shifts utilizing adaptive t-shaped kernels with adaptive dilations.</p> -->

          </div>
        </div>
      </div>
      
  </DIV>
</DIV>

</body>