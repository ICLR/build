<!doctype html>
<html lang="en">
  
  <head>
  <!-- Required meta tags -->
  <meta charset="utf-8">
  <meta name="viewport"
    content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <link rel="stylesheet" href="static/css/main.css">
  <script src="https://d3js.org/d3.v5.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/handlebars.js/4.7.3/handlebars.min.js"
    integrity="sha256-/PJBs6QWvXijOFIX04kZpLb6ZtSQckdOIavLWKKOgXU="
    crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js"
    integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo="
    crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js" integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo" crossorigin="anonymous"></script>
  <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/js/bootstrap.min.js" integrity="sha384-wfSDF2E50Y2D1uUdj0O3uMBJnjuUD4Ih7YwaYd1iqfktj0Uod8GCExl3Og8ifwB6" crossorigin="anonymous"></script>
  <script src="static/js/typeahead.bundle.js"></script>
  <link rel="stylesheet" href="static/css/typeahead.css">
  <link rel="stylesheet"
    href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css"
    integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T"
    crossorigin="anonymous">
  <link rel="stylesheet" href="static/css/lazy_load.css">
  <script src="static/js/lazy_load.js"></script>
  <title> ICLR: Monotonic Multihead Attention </title>
</head>
  <meta name="citation_title" content="Monotonic Multihead Attention" />
  
  <meta name="citation_author" content="Xutai Ma" />
  
  <meta name="citation_author" content="Juan Miguel Pino" />
  
  <meta name="citation_author" content="James Cross" />
  
  <meta name="citation_author" content="Liezl Puzon" />
  
  <meta name="citation_author" content="Jiatao Gu" />
  
  <meta name="citation_publication_date" content="2020/04"/>
  <meta name="citation_conference_title" content="Eighth International Conference on Learning Representations" />
  <meta name="citation_abstract" content="Simultaneous machine translation models start generating a target sequence before they have encoded or read the source sequence. Recent approach for this task either apply a fixed policy on transformer, or a learnable monotonic attention on a weaker recurrent neural network based structure. In this paper, we propose a new attention mechanism, Monotonic Multihead Attention (MMA), which introduced the monotonic attention mechanism to multihead attention. We also introduced two novel interpretable approaches for latency control that are specifically designed for multiple attentions. We apply MMA to the simultaneous machine translation task and demonstrate better latency-quality tradeoffs compared to MILk, the previous state-of-the-art approach.
" />
  <meta name="citation_pdf_url" content="http://www.openreview.net/pdf?id=Hyg96gBKPS" />
  
  <meta name="citation_keywords" content="attention" />
  
  <meta name="citation_keywords" content="machine translation" />
  
  <meta name="citation_keywords" content="transformer" />
  
  <body >
    <!-- NAV -->
    
<script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js" integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/js/bootstrap.min.js" integrity="sha384-wfSDF2E50Y2D1uUdj0O3uMBJnjuUD4Ih7YwaYd1iqfktj0Uod8GCExl3Og8ifwB6" crossorigin="anonymous"></script>
<link href="https://fonts.googleapis.com/css?family=Lato:400,900&display=swap" rel="stylesheet">
<link href="https://fonts.googleapis.com/css?family=Exo" rel='stylesheet'>
<link href="https://fonts.googleapis.com/css?family=Cuprum" rel='stylesheet'>
<script src="https://cdnjs.cloudflare.com/ajax/libs/moment.js/2.24.0/moment.min.js"
  integrity="sha256-4iQZ6BVL4qNKlQ27TExEhBN1HFPvAvAMbFavKKosSWQ="
  crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/moment-timezone/0.5.28/moment-timezone-with-data.min.js"
  integrity="sha256-IWYg4uIC8/erItNXYvLtyYHioRi2zT1TFva8qaAU/ww="
  crossorigin="anonymous"></script>
<nav class="navbar sticky-top navbar-expand-lg navbar-light bg-light mr-auto " id="main-nav" >
  <div class="container">
    <a class="navbar-brand" href="index.html">
    <img class="logo" style='visibility: ' src="static/images/ICLR-logo.png"  width="180px" />
    </a>
    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
    </button>
    <div class="collapse navbar-collapse text-right flex-grow-1" id="navbarNav">
      <ul class="navbar-nav ml-auto">
        
        <li class="nav-item ">
          
          <a class="nav-link" href="index.html">Home</a>
          
        </li>
        
        <li class="nav-item ">
          
          <a class="nav-link" href="calendar.html">Schedule</a>
          
        </li>
        
        <li class="nav-item ">
          
          <a class="nav-link" href="workshops.html">Workshops</a>
          
        </li>
        
        <li class="nav-item ">
          
          <a class="nav-link" href="papers.html">Papers</a>
          
        </li>
        
        <li class="nav-item ">
          
          <a class="nav-link" target="_blank" href="https://iclr.6connex.com/event/VirtualEvent/">
            <nobr>Sponsor Hall <svg class="bi bi-box-arrow-up-right" width="1em" height="1em" viewBox="0 0 16 16" fill="currentColor" xmlns="http://www.w3.org/2000/svg">
    <path fill-rule="evenodd" d="M1.5 13A1.5 1.5 0 003 14.5h8a1.5 1.5 0 001.5-1.5V9a.5.5 0 00-1 0v4a.5.5 0 01-.5.5H3a.5.5 0 01-.5-.5V5a.5.5 0 01.5-.5h4a.5.5 0 000-1H3A1.5 1.5 0 001.5 5v8zm7-11a.5.5 0 01.5-.5h5a.5.5 0 01.5.5v5a.5.5 0 01-1 0V2.5H9a.5.5 0 01-.5-.5z" clip-rule="evenodd"/>
    <path fill-rule="evenodd" d="M14.354 1.646a.5.5 0 010 .708l-8 8a.5.5 0 01-.708-.708l8-8a.5.5 0 01.708 0z" clip-rule="evenodd"/>
</svg></nobr>
          </a>
          
        </li>
        
        <li class="nav-item ">
          
          <a class="nav-link" href="socials.html">Socials</a>
          
        </li>
        
        <li class="nav-item ">
          
          <a class="nav-link" href="chat.html">Chat</a>
          
        </li>
        
        <li class="nav-item ">
          
          <a class="nav-link" href="about.html">Help</a>
          
        </li>
        
      </ul>
    </div>
  </div>
</nav>
    <div class="container">
      <!-- Title -->
      <div class="pp-card m-3" style="">
        <div class="card-header">
          <h2 class="card-title main-title text-center" style="">
            Monotonic Multihead Attention
          </h2>
          <h3 class="card-subtitle mb-2 text-muted text-center">
            
            <a href="papers.html?filter=authors&search=Xutai Ma" class="text-muted">Xutai Ma</a>,
            
            <a href="papers.html?filter=authors&search=Juan Miguel Pino" class="text-muted">Juan Miguel Pino</a>,
            
            <a href="papers.html?filter=authors&search=James Cross" class="text-muted">James Cross</a>,
            
            <a href="papers.html?filter=authors&search=Liezl Puzon" class="text-muted">Liezl Puzon</a>,
            
            <a href="papers.html?filter=authors&search=Jiatao Gu" class="text-muted">Jiatao Gu</a>
            
          </h3>
          <p class="card-text text-center"><span class="">Keywords:</span>
            
            <a href="papers.html?filter=keywords&search=attention" class="text-secondary text-decoration-none">attention</a>,
            
            <a href="papers.html?filter=keywords&search=machine translation" class="text-secondary text-decoration-none">machine translation</a>,
            
            <a href="papers.html?filter=keywords&search=transformer" class="text-secondary text-decoration-none">transformer</a>
            
          </p>
          <div class="text-center p-3">
            <a class="card-link" data-toggle="collapse" role="button" href="#details">
            Abstract
            </a>
            <a class="card-link"  target="_blank" href="http://www.openreview.net/pdf?id=Hyg96gBKPS">
            Paper
            </a>
            
            <a class="card-link"  target="_blank"  href="http://www.openreview.net/forum?id=Hyg96gBKPS">
            Reviews
            </a>
            <!--  -->
            <!-- <a href="38926174" target="_blank"  class="card-link"> -->
            <!--   Slides -->
            <!-- </a> -->
            <!--  -->
            <!-- </div> -->
            <!-- <div class="text-center "> -->
            <a href="chat.html?room=channel/poster_315" target="_blank"  class="card-link">
            Chat
            </a>
          </div>
          <div class=" text-center text-muted text-monospace ">
            
            <div> Wed Session 4  <span class="session_times">(17:00-19:00 GMT)</span>
              [<a href="https://us02web.zoom.us/j/81313018269?pwd=YllBdkdaOUYzTVorNXU4ZGMxVGFDUT09" target="_blank"  class="card-link">Live QA</a>]
              [<a target="_blank" href="webcal://iclr.github.io/iclr-images/calendars/poster_Hyg96gBKPS.0.ics">Cal</a>] 
            </div>
            
            <div> Wed Session 5  <span class="session_times">(20:00-22:00 GMT)</span>
              [<a href="https://us02web.zoom.us/j/84681196151?pwd=M2Urd1JrUmNJZUhBSS8xZzM3Q1I5QT09" target="_blank"  class="card-link">Live QA</a>]
              [<a target="_blank" href="webcal://iclr.github.io/iclr-images/calendars/poster_Hyg96gBKPS.1.ics">Cal</a>] 
            </div>
            
          </div>
        </div>
      </div>
      <div id="details" class="pp-card m-3 collapse">
        <div class="card-body">
          <p class="card-text">
          <div id="abstractExample">
            <span class="font-weight-bold">Abstract:</span>
            Simultaneous machine translation models start generating a target sequence before they have encoded or read the source sequence. Recent approach for this task either apply a fixed policy on transformer, or a learnable monotonic attention on a weaker recurrent neural network based structure. In this paper, we propose a new attention mechanism, Monotonic Multihead Attention (MMA), which introduced the monotonic attention mechanism to multihead attention. We also introduced two novel interpretable approaches for latency control that are specifically designed for multiple attentions. We apply MMA to the simultaneous machine translation task and demonstrate better latency-quality tradeoffs compared to MILk, the previous state-of-the-art approach.

          </div>
          </p>
          <p></p>
        </div>
      </div>
    </div>
    <!-- SlidesLive -->
    <div class="container" style="background-color:white; padding: 0px;">
      <div class="row m-2">
        <div class="col-md-7 col-xs-12 my-auto p-2" >

          <div id="presentation-embed-38926174" class="slp my-auto"></div>
          <script src='https://slideslive.com/embed_presentation.js'></script>
          <script>
            embed = new SlidesLiveEmbed('presentation-embed-38926174', {
            presentationId: '38926174',
            autoPlay: false, // change to true to autoplay the embedded presentation
            verticalEnabled: true,
            verticalWhenWidthLte: 2000,
            allowHiddenControlsWhenPaused: true,
            hideTitle: true
            });
          </script>
        </div>
        <div class="col-md-5 col-xs-12 p-2">
          <div id="gitter" class="slp">
            <center>
              <iframe frameborder="0" src="https://iclr.rocket.chat/channel/poster_315?layout=embedded" height="700px" width="100%" ></iframe>
            </center>
          </div>
        </div>
      </div>
    </div>

    <!-- Recs -->
    <p></p>
    <div  class="container" style="padding-bottom: 30px; padding-top:30px">
      <center>
        <h2> Similar Papers </h2>
      </center>
    </div>
    <p></p>
    <div  class="container" >
      <div class="row">
        
        <div class="col-md-4 col-xs-6">
          <div class="pp-card" >
            <div class="pp-card-header" class="text-muted">
              <a href="poster_rkecJ6VFvr.html" class="text-muted">
                <h5 class="card-title" align="center">Logic and the 2-Simplicial Transformer</h5>
              </a>
              <h6 class="card-subtitle text-muted" align="center">
                
                James Clift,
                
                Dmitry Doryn,
                
                Daniel Murfet,
                
                James Wallbridge,
                
              </h6>
              <center><img class="cards_img" src="https://iclr.github.io/iclr-images/rkecJ6VFvr.png" width="80%"/></center>
            </div>
          </div>
        </div>
        
        <div class="col-md-4 col-xs-6">
          <div class="pp-card" >
            <div class="pp-card-header" class="text-muted">
              <a href="poster_BJg1f6EFDB.html" class="text-muted">
                <h5 class="card-title" align="center">On Identifiability in Transformers</h5>
              </a>
              <h6 class="card-subtitle text-muted" align="center">
                
                Gino Brunner,
                
                Yang Liu,
                
                Damian Pascual,
                
                Oliver Richter,
                
                Massimiliano Ciaramita,
                
                Roger Wattenhofer,
                
              </h6>
              <center><img class="cards_img" src="https://iclr.github.io/iclr-images/BJg1f6EFDB.png" width="80%"/></center>
            </div>
          </div>
        </div>
        
        <div class="col-md-4 col-xs-6">
          <div class="pp-card" >
            <div class="pp-card-header" class="text-muted">
              <a href="poster_BJxWx0NYPr.html" class="text-muted">
                <h5 class="card-title" align="center">Adaptive Structural Fingerprints for Graph Attention Networks</h5>
              </a>
              <h6 class="card-subtitle text-muted" align="center">
                
                Kai Zhang,
                
                Yaokang Zhu,
                
                Jun Wang,
                
                Jie Zhang,
                
              </h6>
              <center><img class="cards_img" src="https://iclr.github.io/iclr-images/BJxWx0NYPr.png" width="80%"/></center>
            </div>
          </div>
        </div>
        
        <div class="col-md-4 col-xs-6">
          <div class="pp-card" >
            <div class="pp-card-header" class="text-muted">
              <a href="poster_BJlZ5ySKPH.html" class="text-muted">
                <h5 class="card-title" align="center">U-GAT-IT: Unsupervised Generative Attentional Networks with Adaptive Layer-Instance Normalization for Image-to-Image Translation</h5>
              </a>
              <h6 class="card-subtitle text-muted" align="center">
                
                Junho Kim,
                
                Minjae Kim,
                
                Hyeonwoo Kang,
                
                Kwang Hee Lee,
                
              </h6>
              <center><img class="cards_img" src="https://iclr.github.io/iclr-images/BJlZ5ySKPH.png" width="80%"/></center>
            </div>
          </div>
        </div>
        
      </div>
    </div>
    <script type="text/javascript">
  $(document).ready(function () {
  
  if (location.hash !== '') {
  $('a[href="' + location.hash + '"]').tab('show');
  }
  
  $("a[data-toggle='tab']").on("shown.bs.tab", function (e) {
  var hash = $(e.target).attr("href");
  if (hash.substr(0,1) == "#") {
  var position = $(window).scrollTop();
  location.replace("#" + hash.substr(1));
  $(window).scrollTop(position);
  }
  });
  
  });
  
  <!-- Global site tag (gtag.js) - Google Analytics -->
</script>
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-163955028-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  
  gtag('config', 'UA-163955028-1');
</script>
<footer class="footer bg-light p-4">
  <div class="container">
    <p class="float-right"><a href="#">Back to Top</a></p>
    <p class="text-center">© 2020 International Conference on Learning Representations</p>
  </div>
</footer>
  </body>
  <script src="static/js/time-extend.js"></script>
<script>
  $(document).ready(()=>{
    add_local_tz('.session_times');
  })
</script>
</html>