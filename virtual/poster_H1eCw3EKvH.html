<!doctype html>
<html lang="en">
  
  <head>
    <!-- Required meta tags -->
    <meta charset="utf-8">
    <meta name="viewport"
          content="width=device-width, initial-scale=1, shrink-to-fit=no">

    <link rel="stylesheet" href="static/css/main.css">

    <script src="https://d3js.org/d3.v5.min.js"></script>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/handlebars.js/4.7.3/handlebars.min.js"
            integrity="sha256-/PJBs6QWvXijOFIX04kZpLb6ZtSQckdOIavLWKKOgXU="
            crossorigin="anonymous"></script>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js"
            integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo="
            crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js" integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo" crossorigin="anonymous"></script>


    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/js/bootstrap.min.js" integrity="sha384-wfSDF2E50Y2D1uUdj0O3uMBJnjuUD4Ih7YwaYd1iqfktj0Uod8GCExl3Og8ifwB6" crossorigin="anonymous"></script>


    <script src="static/js/typeahead.bundle.js"></script>

    <link rel="stylesheet" href="static/css/typeahead.css">
    <link rel="stylesheet"
          href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css"
          integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T"
          crossorigin="anonymous">

    <title> ICLR: On the Weaknesses of Reinforcement Learning for Neural Machine Translation </title>
</head>


  <body >

<!-- NAV -->

<script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js" integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/js/bootstrap.min.js" integrity="sha384-wfSDF2E50Y2D1uUdj0O3uMBJnjuUD4Ih7YwaYd1iqfktj0Uod8GCExl3Og8ifwB6" crossorigin="anonymous"></script>
<link href="https://fonts.googleapis.com/css?family=Lato:400,900&display=swap" rel="stylesheet">
<link href="https://fonts.googleapis.com/css?family=Exo" rel='stylesheet'>
<link href="https://fonts.googleapis.com/css?family=Cuprum" rel='stylesheet'>
<nav class="navbar sticky-top navbar-expand-lg navbar-light bg-light mr-auto " id="main-nav" >
  <div class="container">
    <a class="navbar-brand" href="#">
      <img class="logo" style='visibility: ' src="static/images/ICLR-logo.png"  width="180px" />

    </a>
    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>
    <div class="collapse navbar-collapse text-right flex-grow-1" id="navbarNav">
      <ul class="navbar-nav ml-auto">
        
        <li class="nav-item ">
          <a class="nav-link" href="index.html">Home</a>
        </li>
        
        <li class="nav-item ">
          <a class="nav-link" href="papers.html">Browse</a>
        </li>
        
        <li class="nav-item ">
          <a class="nav-link" href="calendar.html">Schedule</a>
        </li>
        
        <li class="nav-item ">
          <a class="nav-link" href="events.html">Events</a>
        </li>
        
        <li class="nav-item ">
          <a class="nav-link" href="sponsors.html">Sponsors</a>
        </li>
        
        <li class="nav-item ">
          <a class="nav-link" href="paper_vis.html">Extras</a>
        </li>
        
        <li class="nav-item ">
          <a class="nav-link" href="chat.html">Chat</a>
        </li>
        
        <li class="nav-item ">
          <a class="nav-link" href="about.html">About</a>
        </li>
        
        <li class="nav-item ">
          <a class="nav-link" href="faq.html">Help</a>
        </li>
        
      </ul>
    </div>
  </div>
</nav>


<div class="container">

 <!-- Title -->

 <div class="pp-card m-3" style="">
   <div class="card-header">
     <h2 class="card-title main-title text-center" style="">
       On the Weaknesses of Reinforcement Learning for Neural Machine Translation
     </h2>

     <h3 class="card-subtitle mb-2 text-muted text-center">
       
       Leshem Choshen,
       
       Lior Fox,
       
       Zohar Aizenbud,
       
       Omri Abend
       
     </h3>

     <div class="text-center p-3">
       <a class="card-link" data-toggle="collapse" role="button" href="#details">
           Abstract
       </a>

       <a class="card-link"  target="_blank" href="http://www.openreview.net/pdf/05e6f95de9f8eb952e3cbd7f1c21ac9a755cf9d6.pdf">
           Paper
       </a>
       <a class="card-link"  target="_blank"  href="http://www.openreview.net/forum?id=H1eCw3EKvH">
           OpenReview
       </a>

     <!-- </div> -->
     
     <!-- <div class="text-center "> -->
       <a href="" target="_blank"  class="card-link">
           Zoom
       </a>

       <a href="https://iclr.rocket.chat/channel/paper_channel_H1eCw3EKvH" target="_blank"  class="card-link">
           Chat
       </a>

       

       
     </div>
   </div>

   </div>

 <div id="details" class="pp-card m-3 collapse">
   <div class="card-body">
     <p class="card-text">
       <div id="abstractExample">
         <span class="font-weight-bold">Abstract:</span>
         Reinforcement learning (RL) is frequently used to increase performance in text generation tasks,
including machine translation (MT), 
notably through the use of Minimum Risk Training (MRT) and Generative Adversarial Networks (GAN). 
However, little is known about what and how these methods learn in the context of MT. 
We prove that one of the most common RL methods for MT does not optimize the 
expected reward, as well as show that other methods take an infeasibly long time to converge.
In fact, our results suggest that RL practices in MT are likely to improve performance
only where the pre-trained parameters are already close to yielding the correct translation.
Our findings further suggest that observed gains may be due to effects unrelated to the training signal, concretely, changes in the shape of the distribution curve.
       </div>

     </p>

     <p></p>
     <p class="card-text"><span class="font-weight-bold">Keywords:</span>
       
       <a href="keyword_Reinforcement learning.html" class="text-secondary text-decoration-none">Reinforcement learning</a>,
       
       <a href="keyword_MRT.html" class="text-secondary text-decoration-none">MRT</a>,
       
       <a href="keyword_minimum risk training.html" class="text-secondary text-decoration-none">minimum risk training</a>,
       
       <a href="keyword_reinforce.html" class="text-secondary text-decoration-none">reinforce</a>,
       
       <a href="keyword_machine translation.html" class="text-secondary text-decoration-none">machine translation</a>,
       
       <a href="keyword_peakkiness.html" class="text-secondary text-decoration-none">peakkiness</a>,
       
       <a href="keyword_generation.html" class="text-secondary text-decoration-none">generation</a>
       
     </p>
   </div>
 </div>

</div>

<!-- SlidesLive -->



 <div class="container" style="background-color:white; padding: 0px;">
  <div class="row m-2">
    <div class="col-md-7 col-xs-12 my-auto p-2" >
      <div id="presentation-embed-38915748" class="slp my-auto"></div>
      <script src='https://slideslive.com/embed_presentation.js'></script>
      <script>
        embed = new SlidesLiveEmbed('presentation-embed-38915748', {
        presentationId: '38915748',
        autoPlay: false, // change to true to autoplay the embedded presentation
        verticalEnabled: true,
        verticalWhenWidthLte: 2000,
        allowHiddenControlsWhenPaused: true,
        hideTitle: true
        });
      </script>
    </div>

    <div class="col-md-5 col-xs-12 p-2">
        <div id="gitter" class="slp">
          <center>
             <iframe frameborder="0" src="https://iclr.rocket.chat/channel/paper_channel_H1eCw3EKvH?layout=embedded" height="700px" width="100%" ></iframe>
          </center>
        </div>
      </div>
    </div>
  </div>
</div>

  <!-- Recs -->
  <p></p>


  <div  class="container" style="padding-bottom: 30px; padding-top:30px">
  <center>
    <h2> Similar Papers </h2>
</div>
<p></p>

<div  class="container" >
  <div class="row">

      
      <div class="col-md-4 col-xs-6">
        <div class="pp-card" >
          <div class="pp-card-header" class="text-muted">
            <a href="poster_S1xitgHtvS.html" class="text-muted">
              <h5 class="card-title" align="center">Making Sense of Reinforcement Learning and Probabilistic Inference</h5></a>
            <h6 class="card-subtitle text-muted" align="center">
              
              Brendan O&#39;Donoghue,
              
              Ian Osband,
              
              Catalin Ionescu,
              
            </h6>

            <center><img class="cards_img" src="https://iclr.github.io/iclr-images/S1xitgHtvS.png" width="80%"/></center>

            <!-- <p class="card-text"> Popular algorithms that cast &#34;RL as Inference&#34; ignore the role of uncertainty and exploration. We highlight the importance of these issues and present a coherent framework for RL and inference that handles them gracefully.</p> -->

          </div>
        </div>
      </div>
      
      <div class="col-md-4 col-xs-6">
        <div class="pp-card" >
          <div class="pp-card-header" class="text-muted">
            <a href="poster_S1xKd24twB.html" class="text-muted">
              <h5 class="card-title" align="center">SQIL: Imitation Learning via Reinforcement Learning with Sparse Rewards</h5></a>
            <h6 class="card-subtitle text-muted" align="center">
              
              Siddharth Reddy,
              
              Anca D. Dragan,
              
              Sergey Levine,
              
            </h6>

            <center><img class="cards_img" src="https://iclr.github.io/iclr-images/S1xKd24twB.png" width="80%"/></center>

            <!-- <p class="card-text"> A simple and effective alternative to adversarial imitation learning: initialize experience replay buffer with demonstrations, set their reward to +1, set reward for all other data to 0, run Q-learning or soft actor-critic to train.</p> -->

          </div>
        </div>
      </div>
      
      <div class="col-md-4 col-xs-6">
        <div class="pp-card" >
          <div class="pp-card-header" class="text-muted">
            <a href="poster_HJli2hNKDH.html" class="text-muted">
              <h5 class="card-title" align="center">Observational Overfitting in Reinforcement Learning</h5></a>
            <h6 class="card-subtitle text-muted" align="center">
              
              Xingyou Song,
              
              Yiding Jiang,
              
              Stephen Tu,
              
              Yilun Du,
              
              Behnam Neyshabur,
              
            </h6>

            <center><img class="cards_img" src="https://iclr.github.io/iclr-images/HJli2hNKDH.png" width="80%"/></center>

            <!-- <p class="card-text"> We isolate one factor of RL generalization by analyzing the case when the agent only overfits to the observations. We show that architectural implicit regularizations occur in this regime.</p> -->

          </div>
        </div>
      </div>
      
      <div class="col-md-4 col-xs-6">
        <div class="pp-card" >
          <div class="pp-card-header" class="text-muted">
            <a href="poster_Bkl7bREtDr.html" class="text-muted">
              <h5 class="card-title" align="center">AMRL: Aggregated Memory For Reinforcement Learning</h5></a>
            <h6 class="card-subtitle text-muted" align="center">
              
              Jacob Beck,
              
              Kamil Ciosek,
              
              Sam Devlin,
              
              Sebastian Tschiatschek,
              
              Cheng Zhang,
              
              Katja Hofmann,
              
            </h6>

            <center><img class="cards_img" src="https://iclr.github.io/iclr-images/Bkl7bREtDr.png" width="80%"/></center>

            <!-- <p class="card-text"> In Deep RL, order-invariant functions can be used in conjunction with standard memory modules to improve gradient decay and resilience to noise.</p> -->

          </div>
        </div>
      </div>
      
  </DIV>
</DIV>

</body>