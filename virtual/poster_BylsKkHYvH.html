<!doctype html>
<html lang="en">
  
  <head>
    <!-- Required meta tags -->
    <meta charset="utf-8">
    <meta name="viewport"
          content="width=device-width, initial-scale=1, shrink-to-fit=no">

    <link rel="stylesheet" href="static/css/main.css">

    <script src="https://d3js.org/d3.v5.min.js"></script>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/handlebars.js/4.7.3/handlebars.min.js"
            integrity="sha256-/PJBs6QWvXijOFIX04kZpLb6ZtSQckdOIavLWKKOgXU="
            crossorigin="anonymous"></script>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js"
            integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo="
            crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js" integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo" crossorigin="anonymous"></script>


    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/js/bootstrap.min.js" integrity="sha384-wfSDF2E50Y2D1uUdj0O3uMBJnjuUD4Ih7YwaYd1iqfktj0Uod8GCExl3Og8ifwB6" crossorigin="anonymous"></script>


    <script src="static/js/typeahead.bundle.js"></script>

    <link rel="stylesheet" href="static/css/typeahead.css">
    <link rel="stylesheet"
          href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css"
          integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T"
          crossorigin="anonymous">

    <title> ICLR: Why Not to Use Zero Imputation? Correcting Sparsity Bias in Training Neural Networks </title>
</head>


  <body >

<!-- NAV -->

<script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js" integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/js/bootstrap.min.js" integrity="sha384-wfSDF2E50Y2D1uUdj0O3uMBJnjuUD4Ih7YwaYd1iqfktj0Uod8GCExl3Og8ifwB6" crossorigin="anonymous"></script>
<link href="https://fonts.googleapis.com/css?family=Lato:400,900&display=swap" rel="stylesheet">
<link href="https://fonts.googleapis.com/css?family=Exo" rel='stylesheet'>
<link href="https://fonts.googleapis.com/css?family=Cuprum" rel='stylesheet'>
<nav class="navbar sticky-top navbar-expand-lg navbar-light bg-light mr-auto " id="main-nav" >
  <div class="container">
    <a class="navbar-brand" href="#">
      <img class="logo" style='visibility: ' src="static/images/ICLR-logo.png"  width="180px" />

    </a>
    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>
    <div class="collapse navbar-collapse text-right flex-grow-1" id="navbarNav">
      <ul class="navbar-nav ml-auto">
        
        <li class="nav-item ">
          <a class="nav-link" href="index.html">Home</a>
        </li>
        
        <li class="nav-item ">
          <a class="nav-link" href="papers.html">Browse</a>
        </li>
        
        <li class="nav-item ">
          <a class="nav-link" href="calendar.html">Schedule</a>
        </li>
        
        <li class="nav-item ">
          <a class="nav-link" href="events.html">Events</a>
        </li>
        
        <li class="nav-item ">
          <a class="nav-link" href="sponsors.html">Sponsors</a>
        </li>
        
        <li class="nav-item ">
          <a class="nav-link" href="paper_vis.html">Extras</a>
        </li>
        
        <li class="nav-item ">
          <a class="nav-link" href="chat.html">Chat</a>
        </li>
        
        <li class="nav-item ">
          <a class="nav-link" href="about.html">About</a>
        </li>
        
        <li class="nav-item ">
          <a class="nav-link" href="faq.html">Help</a>
        </li>
        
      </ul>
    </div>
  </div>
</nav>


<div class="container">

 <!-- Title -->

 <div class="card m-3" style="">
   <div class="card-header">
     <h3 class="card-title main-title text-center" style="">
       Why Not to Use Zero Imputation? Correcting Sparsity Bias in Training Neural Networks
     </h3>

     <h5 class="card-subtitle mb-2 text-muted text-center">
       
       Joonyoung Yi,
       
       Juhyuk Lee,
       
       Kwang Joon Kim,
       
       Sung Ju Hwang,
       
       Eunho Yang
       
     </h5>

     <center class="p-3">
       <a class="card-link" data-toggle="collapse" role="button" href="#details">
         <button class="btn btn-outline-secondary">
           Abstract
         </button>
       </a>


       <a class="card-link"  target="_blank" href="http://www.openreview.net/pdf/cc3e53cdbd7be11a0296d5a730ecb98d53f296fa.pdf">
         <button class="btn btn-outline-secondary">
           Paper
         </button>
       </a>
       <a class="card-link"  target="_blank"  href="http://www.openreview.net/forum?id=BylsKkHYvH">
         <button class="btn btn-outline-secondary">
           OpenReview
         </button>
       </a>

       <a href="" target="_blank"  class="card-link">
         <button class="btn btn-outline-secondary">
           Zoom
         </button>
       </a>

       <a href="https://iclr.rocket.chat/channel/paper_channel_BylsKkHYvH" target="_blank"  class="card-link">
         <button class="btn btn-outline-secondary">
           Chat
         </button>
       </a>

       
       <a href="https://github.com/JoonyoungYi/sparsity-normalization" target="_blank"  class="card-link">
         <button class="btn btn-outline-secondary">
           Code
         </button>
       </a>
       

       
     </center>

   </div>
 </div>

 <div id="details" class="card m-3 collapse" style=" box-shadow: 2px 2px 14px 0px rgba(204, 204, 204, 1);">
   <div class="card-body">
     <p class="card-text">
       <div id="abstractExample">
         <span class="font-weight-bold">Abstract:</span>
         Handling missing data is one of the most fundamental problems in machine learning. Among many approaches, the simplest and most intuitive way is zero imputation, which treats the value of a missing entry simply as zero. However, many studies have experimentally confirmed that zero imputation results in suboptimal performances in training neural networks. Yet, none of the existing work has explained what brings such performance degradations. In this paper, we introduce the variable sparsity problem (VSP), which describes a phenomenon where the output of a predictive model largely varies with respect to the rate of missingness in the given input, and show that it adversarially affects the model performance. We first theoretically analyze this phenomenon and propose a simple yet effective technique to handle missingness, which we refer to as Sparsity Normalization (SN), that directly targets and resolves the VSP. We further experimentally validate SN on diverse benchmark datasets, to show that debiasing the effect of input-level sparsity improves the performance and stabilizes the training of neural networks.
       </div>

     </p>

     <p></p>
     <p class="card-text"><span class="font-weight-bold">Keywords:</span>
       
       <a href="keyword_Missing Data.html" class="text-secondary text-decoration-none">Missing Data</a>,
       
       <a href="keyword_Collaborative Filtering.html" class="text-secondary text-decoration-none">Collaborative Filtering</a>,
       
       <a href="keyword_Health Care.html" class="text-secondary text-decoration-none">Health Care</a>,
       
       <a href="keyword_Tabular Data.html" class="text-secondary text-decoration-none">Tabular Data</a>,
       
       <a href="keyword_High Dimensional Data.html" class="text-secondary text-decoration-none">High Dimensional Data</a>,
       
       <a href="keyword_Deep Learning.html" class="text-secondary text-decoration-none">Deep Learning</a>,
       
       <a href="keyword_Neural Networks.html" class="text-secondary text-decoration-none">Neural Networks</a>
       
     </p>
   </div>
 </div>

</div>

<!-- SlidesLive -->



 <div class="jumbotron" style="background-color:white; padding: 0px;">
  <div class="row m-2">
    <div class="col-md-7 col-xs-12 my-auto p-2" >
      <div id="presentation-embed-38915748" class="slp my-auto"></div>
      <script src='https://slideslive.com/embed_presentation.js'></script>
      <script>
        embed = new SlidesLiveEmbed('presentation-embed-38915748', {
        presentationId: '38915748',
        autoPlay: false, // change to true to autoplay the embedded presentation
        verticalEnabled: true,
        verticalWhenWidthLte: 2000,
        allowHiddenControlsWhenPaused: true,
        hideTitle: true
        });
      </script>
    </div>

    <div class="col-md-5 col-xs-12 p-2">
        <div id="gitter" class="slp">
          <center>
             <iframe frameborder="0" src="https://iclr.rocket.chat/channel/paper_channel_BylsKkHYvH?layout=embedded" height="700px" width="100%" ></iframe>
          </center>
        </div>
      </div>
    </div>
  </div>
</div>

  <!-- Recs -->
  <p></p>


  <div  class="container" style="padding-bottom: 30px; padding-top:30px">
  <center>
    <h2> Similar Papers </h2>
</div>
<p></p>

<div  class="container" >
  <div class="row">

      
      <div class="col-md-4 col-xs-6">
        <div class="pp-card" >
          <div class="pp-card-header" class="text-muted">
            <a href="poster_Syx1DkSYwB.html" class="text-muted">
              <h5 class="card-title" align="center">Variance Reduction With Sparse Gradients</h5></a>
            <h6 class="card-subtitle text-muted" align="center">
              
              Melih Elibol,
              
              Lihua Lei,
              
              Michael I. Jordan,
              
            </h6>

            <center><img class="cards_img" src="https://iclr.github.io/iclr-images/Syx1DkSYwB.png" width="80%"/></center>

            <!-- <p class="card-text"> We use sparsity to improve the computational complexity of variance reduction methods.</p> -->

          </div>
        </div>
      </div>
      
      <div class="col-md-4 col-xs-6">
        <div class="pp-card" >
          <div class="pp-card-header" class="text-muted">
            <a href="poster_rylBK34FDS.html" class="text-muted">
              <h5 class="card-title" align="center">DeepHoyer: Learning Sparser Neural Network with Differentiable Scale-Invariant Sparsity Measures</h5></a>
            <h6 class="card-subtitle text-muted" align="center">
              
              Huanrui Yang,
              
              Wei Wen,
              
              Hai Li,
              
            </h6>

            <center><img class="cards_img" src="https://iclr.github.io/iclr-images/rylBK34FDS.png" width="80%"/></center>

            <!-- <p class="card-text"> We propose almost everywhere differentiable and scale invariant regularizers for DNN pruning, which can lead to supremum sparsity through standard SGD training.</p> -->

          </div>
        </div>
      </div>
      
      <div class="col-md-4 col-xs-6">
        <div class="pp-card" >
          <div class="pp-card-header" class="text-muted">
            <a href="poster_HJx8HANFDH.html" class="text-muted">
              <h5 class="card-title" align="center">Four Things Everyone Should Know to Improve Batch Normalization</h5></a>
            <h6 class="card-subtitle text-muted" align="center">
              
              Cecilia Summers,
              
              Michael J. Dinneen,
              
            </h6>

            <center><img class="cards_img" src="https://iclr.github.io/iclr-images/HJx8HANFDH.png" width="80%"/></center>

            <!-- <p class="card-text"> Four things that improve batch normalization across all batch sizes</p> -->

          </div>
        </div>
      </div>
      
      <div class="col-md-4 col-xs-6">
        <div class="pp-card" >
          <div class="pp-card-header" class="text-muted">
            <a href="poster_SyxIWpVYvr.html" class="text-muted">
              <h5 class="card-title" align="center">Input Complexity and Out-of-distribution Detection with Likelihood-based Generative Models</h5></a>
            <h6 class="card-subtitle text-muted" align="center">
              
              Joan Serrà,
              
              David Álvarez,
              
              Vicenç Gómez,
              
              Olga Slizovskaia,
              
              José F. Núñez,
              
              Jordi Luque,
              
            </h6>

            <center><img class="cards_img" src="https://iclr.github.io/iclr-images/SyxIWpVYvr.png" width="80%"/></center>

            <!-- <p class="card-text"> We pose that generative models&#39; likelihoods are excessively influenced by the input&#39;s complexity, and propose a way to compensate it when detecting out-of-distribution inputs</p> -->

          </div>
        </div>
      </div>
      
  </DIV>
</DIV>

</body>